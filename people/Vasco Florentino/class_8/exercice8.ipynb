{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis and Visualization of Complex Agro-Environmental Data\n",
    "---\n",
    "## Ordination Analysis\n",
    "\n",
    "### 1. Principal Component Analysis (PCA)\n",
    "\n",
    "PCA is one of the most popular methods in Exploratory Data Analysis, as a means to explore, through visualizations, the relationships among a set of objects (sampling units, indiv√≠duals, experimental plots, ...) in relation to a set of variables (different measurements undertaken at each object). It is also often use to create few orthogonal latent variables that retain most of the original data variance. These latent variables are specially usefull in regression and machine learning, to avoid problems derived from multicolinearity among explanatory variables.\n",
    "\n",
    "In `python` PCA is implemented in the `sklearn.cluster` module, in the `sklearn.decomposition.PCA` function (check [here](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html))\n",
    "\n",
    "### 1.1 Data preparation and exploration\n",
    "\n",
    "To run the analysis you first need to import necessary modules and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will analyse the dataset `winequality_red.csv` available [here](https://www.kaggle.com/datasets/sgus1318/winedata?resource=download) with 12 variables (columns) related with wine quality characteristics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wine = pd.read_csv('winequality_red.csv')\n",
    "df_wine.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove variable `quality`, as this variable is derived from all the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_wine.iloc[:, 0:11]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check multicolinearity in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrdot(*args, **kwargs):\n",
    "    corr_r = args[0].corr(args[1], 'pearson')\n",
    "    corr_text = f\"{corr_r:2.2f}\".replace(\"0.\", \".\")\n",
    "    ax = plt.gca()\n",
    "    ax.set_axis_off()\n",
    "    marker_size = abs(corr_r) * 10000\n",
    "    ax.scatter([.5], [.5], marker_size, [corr_r], alpha=0.6, cmap=\"coolwarm\",\n",
    "               vmin=-1, vmax=1, transform=ax.transAxes)\n",
    "    font_size = abs(corr_r) * 40 + 5\n",
    "    ax.annotate(corr_text, [.5, .5,],  xycoords=\"axes fraction\",\n",
    "                ha='center', va='center', fontsize=font_size)\n",
    "\n",
    "sns.set(style='white', font_scale=1.6)\n",
    "\n",
    "g = sns.PairGrid(df, aspect=1.4, diag_sharey=False)\n",
    "g.map_lower(sns.regplot, lowess=True, ci=False, line_kws={'color': 'black'})\n",
    "g.map_diag(sns.histplot, kde_kws={'color': 'black'})\n",
    "g.map_upper(corrdot);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With a few exceptions, variables are not so correlated with each other. This means that probably it will be difficult that the first Principal Components will retain most of the variance in the data.\n",
    "\n",
    "### 1.2 Data standardization\n",
    "\n",
    "Now we need to scale the variables before conducting the analysis to avoid misleading PCA results due to units differences. The variables need to be standardize, so that the mean of each variable = 0 and the variance = 1. We will use the `StandardScaler` class of `scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "wine_scaled = StandardScaler().fit_transform(df)\n",
    "# As a result, we obtained a two-dimensional NumPy array. We can convert it to a pandas DataFrame for a better display.\n",
    "df_scaled = pd.DataFrame(data=wine_scaled, \n",
    "                                columns=df.columns)\n",
    "df_scaled.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now ready to run PCA.\n",
    "\n",
    "### 1.3 Select the number of Principal Components (PC)\n",
    "\n",
    "To select the number of PC, usually a Scree plot is produced to visualize the percentage of explained variance or the eigenvalues per component. Either the `elbow rule` - retaining all components before the point where the curve flattens out - or the `Kaiser's rule` - retaining components whose eigenvalues are greater than 1.\n",
    "\n",
    "First we run PCA for 11 components (number of original variables)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=11)\n",
    "pca.fit_transform(df_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have run the PCA and now we can extract the proportion of explained variance and the eigenvalues as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "eigenvalues = pca.explained_variance_ # eigenvalues\n",
    "prop_var = pca.explained_variance_ratio_ # proportion of explained variance"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the scree plot with proportion of variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_numbers = np.arange(pca.n_components_) + 1\n",
    " \n",
    "plt.plot(PC_numbers, \n",
    "         prop_var,\n",
    "         'ro-')\n",
    "plt.title('Scree Plot', fontsize=20)\n",
    "plt.ylabel('Proportion of Variance', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Elbow rule` is not easy to apply to our data.\n",
    "We can try to apply the `Kaiser's rule` and for that we need to plot the eigenvalues instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PC_numbers = np.arange(pca.n_components_) + 1\n",
    " \n",
    "plt.plot(PC_numbers, \n",
    "         eigenvalues,\n",
    "         'ro-')\n",
    "plt.title('Scree Plot', fontsize=20)\n",
    "plt.ylabel('Eigenvalues', fontsize=16)\n",
    "plt.axhline(y=1, color='r', \n",
    "            linestyle='--')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to this rule, 4 components should be retained. However, just for the purpose of ilustrating how to produce a PCA biplot vlisualization, we will retain only two components.\n",
    "\n",
    "### 1.4 Visualize and interpret the PCA biplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "PC = pca.fit_transform(df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_wine = pd.DataFrame(data = PC, \n",
    "                            columns = ['PC1', 'PC2'])\n",
    "pca_wine.head(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"score\" - Scores of each object (for each PC) - PC\n",
    "# \"coef\" - PCA variable loadings (for each PC) - pca.components_\n",
    "# labels - name of variables - list(df.columns)\n",
    "\n",
    "\n",
    "def biplot(score,coef,labels=None): \n",
    " \n",
    "    xs = score[:,0] # PC1 object scores\n",
    "    ys = score[:,1] # PC2 object scores \n",
    "    n = coef.shape[0] # number of dimensions (2)\n",
    "    scalex = 1.0/(xs.max() - xs.min()) # to rescale scores\n",
    "    scaley = 1.0/(ys.max() - ys.min()) # to rescale scores\n",
    "    plt.scatter(xs * scalex,ys * scaley,\n",
    "                s=6, \n",
    "                color='blue') # scatter plot using rescaled object scores\n",
    " \n",
    "    for i in range(n):\n",
    "        plt.arrow(0, 0, coef[i,0], \n",
    "                  coef[i,1],color = 'red',\n",
    "                  head_width=0.01,\n",
    "                  alpha = 0.5) # plot arrows for each variable\n",
    "        plt.text(coef[i,0]* 1.15, \n",
    "                 coef[i,1] * 1.15, \n",
    "                 labels[i], \n",
    "                 color = 'red', \n",
    "                 ha = 'center', \n",
    "                 va = 'center') # variable labels for each arrow\n",
    " \n",
    "    plt.xlabel(\"PC{}\".format(1))\n",
    "    plt.ylabel(\"PC{}\".format(2))    \n",
    " \n",
    " \n",
    "    plt.figure()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Biplot of PCA')\n",
    " \n",
    "biplot(PC, \n",
    "       np.transpose(pca.components_), \n",
    "       list(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same biplot but with colors expressing wine quality\n",
    "\n",
    "PC1 = pca_wine['PC1']/(pca_wine['PC1'].max() - pca_wine['PC1'].min())\n",
    "PC2 = pca_wine['PC2']/(pca_wine['PC2'].max() - pca_wine['PC2'].min())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('Biplot of PCA')\n",
    "sns.scatterplot(x=PC1,\n",
    "              y=PC2,\n",
    "              hue = df_wine['quality'].tolist(),\n",
    "              linewidth=0,\n",
    "              )\n",
    "\n",
    "n = np.transpose(pca.components_).shape[0] # number of dimensions (2)\n",
    "for i in range(n):\n",
    "        plt.arrow(0, 0, np.transpose(pca.components_)[i,0], \n",
    "                  np.transpose(pca.components_)[i,1], \n",
    "                  color = (0.1, 0.1, 0.1, 0.8),\n",
    "                  head_width=0.02) # plot arrows for each variable\n",
    "        plt.text(np.transpose(pca.components_)[i,0]* 1.15, \n",
    "                 np.transpose(pca.components_)[i,1] * 1.15, \n",
    "                 list(df.columns)[i], \n",
    "                 color = (0.1, 0.1, 0.1, 0.8), \n",
    "                 ha = 'center', \n",
    "                 va = 'center') # variable labels for each arrow\n",
    "plt.legend(title='Wine quality')\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Running PCA from scratch\n",
    "\n",
    "The code that follows computes the same PCA but without depending on any specific PCA function.\n",
    "\n",
    "NOTE: The first step - variable standardization - was already performed\n",
    "\n",
    "Step 2: compute the covariance matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def covariance(x): \n",
    "    return (x.T @ x)/(x.shape[0]-1)\n",
    "\n",
    "cov_mat = covariance(df_scaled) # np.cov(X_std.T)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Compute the eigenvectors and eigenvalues of the covariance matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import eig\n",
    "\n",
    "# Eigendecomposition of covariance matrix\n",
    "eig_vals, eig_vecs = eig(cov_mat)\n",
    "\n",
    "# Adjusting the eigenvectors (loadings) that are largest in absolute value to be positive\n",
    "max_abs_idx = np.argmax(np.abs(eig_vecs), axis=0)\n",
    "signs = np.sign(eig_vecs[max_abs_idx, range(eig_vecs.shape[0])])\n",
    "eig_vecs = eig_vecs*signs[np.newaxis,:]\n",
    "eig_vecs = eig_vecs.T\n",
    "\n",
    "print('Eigenvalues \\n', eig_vals)\n",
    "print('Eigenvectors \\n', eig_vecs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Rearrange the eigenvectors and eigenvalues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of eigenvalue and eigenvector tuples\n",
    "eig_pairs = [(np.abs(eig_vals[i]), eig_vecs[i,:]) for i in range(len(eig_vals))]\n",
    "\n",
    "# Sort the tuples from the highest to the lowest eigenvalues\n",
    "eig_pairs.sort(key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# For further usage\n",
    "eig_vals_sorted = np.array([x[0] for x in eig_pairs])\n",
    "eig_vecs_sorted = np.array([x[1] for x in eig_pairs])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 5: Select the number of PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select top k eigenvectors\n",
    "k = 2\n",
    "W = eig_vecs_sorted[:k, :] # Projection matrix\n",
    "\n",
    "eig_vals_total = sum(eig_vals)\n",
    "explained_variance = [(i / eig_vals_total)*100 for i in eig_vals_sorted]\n",
    "explained_variance = np.round(explained_variance, 2)\n",
    "cum_explained_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print('Explained variance: {}'.format(explained_variance))\n",
    "print('Cumulative explained variance: {}'.format(cum_explained_variance))\n",
    "\n",
    "plt.plot(np.arange(1, 11 + 1), cum_explained_variance, '-o')\n",
    "plt.xticks(np.arange(1,11+1))\n",
    "plt.xlabel('Number of components')\n",
    "plt.ylabel('Cumulative explained variance');\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 6: Project the data with a biplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "X_proj = df_scaled.dot(W.T)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create biplot\n",
    "PC1 = X_proj.iloc[:, 0]/(X_proj.iloc[:, 0].max() - X_proj.iloc[:, 0].min())\n",
    "PC2 =X_proj.iloc[:, 1]/(X_proj.iloc[:, 1].max() - X_proj.iloc[:, 1].min())\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.title('2 components, captures {:.1f} of total variation'.format(cum_explained_variance[1]))\n",
    "\n",
    "sns.scatterplot(x=PC1,\n",
    "              y=PC2,\n",
    "              hue = df_wine['quality'].tolist(),\n",
    "              linewidth=0,\n",
    "              )\n",
    "\n",
    "n = np.transpose(pca.components_).shape[0] # number of dimensions (2)\n",
    "for i in range(n):\n",
    "        plt.arrow(0, 0, np.transpose(pca.components_)[i,0], \n",
    "                  np.transpose(pca.components_)[i,1], \n",
    "                  color = (0.1, 0.1, 0.1, 0.8),\n",
    "                  head_width=0.02) # plot arrows for each variable\n",
    "        plt.text(np.transpose(pca.components_)[i,0]* 1.15, \n",
    "                 np.transpose(pca.components_)[i,1] * 1.15, \n",
    "                 list(df.columns)[i], \n",
    "                 color = (0.1, 0.1, 0.1, 0.8), \n",
    "                 ha = 'center', \n",
    "                 va = 'center') # variable labels for each arrow\n",
    "plt.legend(title='Wine quality')\n",
    "plt.xlabel(\"PC{}\".format(1))\n",
    "plt.ylabel(\"PC{}\".format(2))\n",
    "\n",
    "plt.xlabel('PC1'); plt.xticks([])\n",
    "plt.ylabel('PC2'); plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Multidimensional Scaling (MDS) and Non-Metric Multidimensional Scaling\n",
    "\n",
    "The aim of this ordination method is to project objects from a higher-dimensional space to a lower-dimensional space while preserving the relative distances between those points as much as possible. PCoA (also known as MDS) can be viewed as a generalization of PCA that allows a much wider range of dissimilarity measures to be used (dissimilarities among objects in PCA are euclidean distances).\n",
    "\n",
    "MDS and NMDS are implemented in the `mds()` function of `sklearn.manifold` module.\n",
    "\n",
    "#### 1.1 Import modules and functions\n",
    "\n",
    "Import necessary modules and functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import MDS\n",
    "from sklearn.metrics.pairwise import manhattan_distances, euclidean_distances\n",
    "from matplotlib.offsetbox import OffsetImage, AnnotationBbox"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 Run MDS\n",
    "\n",
    "We will run MDS using the same wine dataset used in PCA. We will use the standardized data (df_scaled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by default the 'mds' argument is set to 'True', which means it will run a MDS\n",
    "# Euclidean distances are also the default \n",
    "# Only two axis are extracted by default\n",
    "mds = MDS(random_state=0, normalized_stress = False) \n",
    "mds_transf = mds.fit_transform(df_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=mds_transf[:,0],\n",
    "              y=mds_transf[:,1],\n",
    "              hue = df_wine['quality'].tolist(),\n",
    "              linewidth=0,\n",
    "              )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may use other distances to compute MDS. For that we need to run the function using a dissimilarity matrix as the input.\n",
    "Let's try running MDS with Manhattan dissimilarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_manhattan = manhattan_distances(df_scaled)\n",
    "mds = MDS(dissimilarity='precomputed', random_state=0, normalized_stress = False)\n",
    "mds_transf2 = mds.fit_transform(dist_manhattan)\n",
    "\n",
    "sns.scatterplot(x=mds_transf2[:,0],\n",
    "              y=mds_transf2[:,1],\n",
    "              hue = df_wine['quality'].tolist(),\n",
    "              linewidth=0,\n",
    "              )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.3 Run NMDS\n",
    "\n",
    "Non-Metric Multidimensional Scaling solved many issues of MDS, namely the computation of the meaured used to evaluate how much the distances in the MDS are related with the original dissimilarities. This methods is based on ranks of distances among objects, so the aim is to retain the relative position of objects from each other, not their distances. It is based on a iterative trial & error algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmds = MDS(n_components=5, random_state=0, metric = False, normalized_stress=\"auto\") # 5 components extracted so that stress is > 0.2\n",
    "nmds_transf = nmds.fit_transform(df_scaled)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The match between object distance in MDS and the original dissimilarities is given by the stress, which is proportional to the residuals of that relationship. The stress of an MDS with a perfect match between MDS distances and original dissimilarities will be closer to zero. \n",
    "\n",
    "As a rule of thumb, an NMDS ordination with a stress value around or above 0.2 is deemed suspect and a stress value approaching 0.3 indicates that the ordination is arbitrary. Stress values equal to or below 0.1 are considered fair, while values equal to or below 0.05 indicate good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.18593492317631086\n"
     ]
    }
   ],
   "source": [
    "stress = nmds.stress_\n",
    "print(stress)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(x=nmds_transf[:,0],\n",
    "              y=nmds_transf[:,1],\n",
    "              hue = df_wine['quality'].tolist(),\n",
    "              linewidth=0,\n",
    "              )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to use the method with dissimilarities other than Euclidean distances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NMDS based on Manhattan distances\n",
    "nmds2 = MDS(metric = False, random_state=0, normalized_stress = 'auto', dissimilarity='precomputed')\n",
    "nmds_transf2 = nmds2.fit_transform(dist_manhattan)\n",
    "\n",
    "sns.scatterplot(x=nmds_transf2[:,0],\n",
    "              y=nmds_transf2[:,1],\n",
    "              hue = df_wine['quality'].tolist(),\n",
    "              linewidth=0,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stress = nmds2.stress_\n",
    "print(stress)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
